{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "![Transformer Architectural Diagram](https://d2l.ai/_images/transformer.svg)\n",
    "\n",
    "This notebook implements the right hand side of this diagram. The `DecoderTransformer` is constructed such that it created an embedding and positional encoding of the input, which are combined before it enters into the `Block`s of `MultiheadAttention` networks. The output of the `MultiheadAttention` is passed through a fully connected MLP to produce the final output. \n",
    "\n",
    "What makes the implemented transformer a `DecoderTransformer` rather than an encoder is the inclusion of the following line in the attention `Head`: `self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))`. This line ensures that the tokens only receive information from tokens that occur before the current token being processed. In other words, it ensures that they only see the past and not into the future. \n",
    "\n",
    "## Optimisations\n",
    "### Layer Norm\n",
    "- Mention the skip connections and dropout for the improved training of deep networks\n",
    "\n",
    "### Skip Connections\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "torch.manual_seed(1337)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64 # How many sequences to process at in parallel\n",
    "BLOCK_SIZE = 256 # What is the maximum context length for predictions?\n",
    "MAX_ITERATIONS = 101\n",
    "LEARNING_RATE = 3e-4\n",
    "EVAL_EVERY = 100\n",
    "N_EMBEDDING_DIMENSIONS = 384\n",
    "N_HEAD = 6\n",
    "N_LAYER = 6\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Attention Head\n",
    " - Talk about the upper traingular portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(N_EMBEDDING_DIMENSIONS, head_size, bias=False)\n",
    "        self.key = nn.Linear(N_EMBEDDING_DIMENSIONS, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBEDDING_DIMENSIONS, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * (self.head_size**-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # This makes it a decoder block\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        \n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "- How does going to \"multi head\" improve things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(N_EMBEDDING_DIMENSIONS, N_EMBEDDING_DIMENSIONS) # Added for skip connections\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), # Multiply by 4 to copy the Transformer paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed), # Projection layer added for skip connections\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = MultiHead(n_head, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: The x + self.self_attention(x) is the residual connection/skip connection\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embedding_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_size, N_HEAD) for _ in range(N_LAYER)])\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx[:, -BLOCK_SIZE:] # Crop to block size\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "val_data = None\n",
    "\n",
    "def load_dataset(path: str) -> str:\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_vocab(text: str) -> list:\n",
    "    return sorted(list(set(text)))\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    assert split in ['train', 'val']\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(0, len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_EVERY)\n",
    "        for k in range(100):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load things\n",
    "dataset = load_dataset('../datasets/tiny-shakespeare.txt')\n",
    "vocab = get_vocab(dataset)\n",
    "\n",
    "stoi = {u:i for i, u in enumerate(vocab)}\n",
    "itos = {i:u for i, u in enumerate(vocab)}\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "decode = lambda x: ''.join([itos[c] for c in x])\n",
    "\n",
    "data = torch.tensor(encode(dataset), dtype=torch.long).to(DEVICE)\n",
    "train_val_ratio = int(0.9 * len(data))\n",
    "train_data = data[:train_val_ratio]\n",
    "val_data = data[train_val_ratio:]\n",
    "\n",
    "model = DecoderTransformer(vocab_size=len(vocab), \n",
    "                           embedding_size=N_EMBEDDING_DIMENSIONS, \n",
    "                           block_size=BLOCK_SIZE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.285, val loss 4.282\n",
      "Step 100: train loss 2.473, val loss 2.490\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for iters in range(MAX_ITERATIONS):\n",
    "    if iters % EVAL_EVERY == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Step {iters}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athqissthetanchemave mashimen tand, cevont bld yomuthy daime my towindss t byofflir d t's ene.\n",
      "Thofind bero gh ns ourerrs ve ttow.\n",
      "\n",
      "I thomeder;\n",
      "K:\n",
      "Me ld, ted bout adowhoitheith t, bo, fot g. hice's ithe Herit d n's O:\n",
      "Courdon ben o s PA imou trospangin; gte\n"
     ]
    }
   ],
   "source": [
    "input_idx = torch.zeros(1, 1, dtype=torch.long, device=DEVICE) # start with a single token {0: '\\n'}\n",
    "print(decode(model.generate(input_idx, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
