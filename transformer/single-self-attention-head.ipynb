{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Self Attention Head\n",
    "Given a set of tokens, the self attention solves the problem of working out by how much a token depends on another in a sequence. This notebook starts with the most basic case of a decoder attention block (one where tokens can't see those that come after it in a sequence), and slowly improves it to one that can be used in a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2044171d10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakest Case\n",
    "The code below creates an exmaple input in the shape `[B, T, C]`, where `B` refers to the number of batches, `T` the number of tokens, and `C` the dimensionality of a token.\n",
    "\n",
    "In a decoder attention head, the model can't see into the future, so at each stage it wants the information of the data that comes before it. In this weakest case, this is achieved through averaging and some for loops. \n",
    "\n",
    "Each row of `x_bag_of_words[0]` contains the average of the rows up to an including the one being processed (`t`th row) of `x[0]`. So the final row is the complete average of all the rows of `x[0]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "x_bag_of_words = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] # (t, C)\n",
    "        x_bag_of_words[b, t] = torch.mean(x_prev, dim=0)\n",
    "\n",
    "display(x[0])\n",
    "display(x_bag_of_words[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Case\n",
    "The above implementation is based on `for` loops which are slow. We can make the code much more efficient using matrix multiplication. The improved case is built up slowly across a number of cells. \n",
    "\n",
    "Below, the code shows the basic matrix multiplication, where $C$ contains the sums of the columns in $B$ (as $A$ is all ones). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Basic mat mul\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print('---')\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print('---')\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering back to the `for` loop implementation, we only want to incorporate data from the token being processed and those that come before it. To achieve this using matrix multiplication, we use a lower triangular matrix. In the code below, $C$ is given by $A \\times B$ but as $A$ is lower-triangular, the values only incorporate those of the rows above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "old c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "---\n",
      "new c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Mat mul with lower triangular matrix\n",
    "torch.manual_seed(42)\n",
    "new_a = torch.tril(torch.ones(3, 3))\n",
    "new_c = new_a @ b\n",
    "\n",
    "print(\"new a=\")\n",
    "print(new_a)\n",
    "print('---')\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print('---')\n",
    "print(\"old c=\")\n",
    "print(c)\n",
    "print('---')\n",
    "print(\"new c=\")\n",
    "print(new_c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above code is only summing, and isn't taking the average, as used in the for loop example. Let's fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newer a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "newer c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "---\n",
      "original c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Mat mul with lower triangular matrix\n",
    "torch.manual_seed(42)\n",
    "newer_a = torch.tril(torch.ones(3, 3))\n",
    "newer_a = newer_a / torch.sum(newer_a, dim=1, keepdim=True)\n",
    "newer_c = newer_a @ b\n",
    "\n",
    "print(\"newer a=\")\n",
    "print(newer_a)\n",
    "print('---')\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print('---')\n",
    "print(\"newer c=\")\n",
    "print(newer_c)\n",
    "print('---')\n",
    "print(\"original c=\")\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with the For Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1=\n",
      "tensor([[ 1.9269,  1.4873],\n",
      "        [ 1.4138, -0.3091],\n",
      "        [ 1.1687, -0.6176],\n",
      "        [ 0.8657, -0.8644],\n",
      "        [ 0.5422, -0.3617],\n",
      "        [ 0.3864, -0.5354],\n",
      "        [ 0.2272, -0.5388],\n",
      "        [ 0.1027, -0.3762]])\n",
      "---\n",
      "y2=\n",
      "tensor([[ 1.9269,  1.4873],\n",
      "        [ 1.4138, -0.3091],\n",
      "        [ 1.1687, -0.6176],\n",
      "        [ 0.8657, -0.8644],\n",
      "        [ 0.5422, -0.3617],\n",
      "        [ 0.3864, -0.5354],\n",
      "        [ 0.2272, -0.5388],\n",
      "        [ 0.1027, -0.3762]])\n",
      "Tensors are equal: True\n"
     ]
    }
   ],
   "source": [
    "def for_loop_based(x):\n",
    "    B, T, C = x.shape\n",
    "    x_bag_of_words = torch.zeros((B, T, C))\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            x_prev = x[b, :t+1] # (t, C)\n",
    "            x_bag_of_words[b, t] = torch.mean(x_prev, dim=0)\n",
    "\n",
    "    return x_bag_of_words\n",
    "\n",
    "\n",
    "def matrix_based(x):\n",
    "    B, T, C = x.shape\n",
    "    weights = torch.tril(torch.ones(T, T))\n",
    "    weights = weights / torch.sum(weights, dim=1, keepdim=True)\n",
    "    return weights @ x\n",
    "\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "y1 = for_loop_based(x)\n",
    "y2 = matrix_based(x)\n",
    "\n",
    "print(\"y1=\")\n",
    "print(y1[0])\n",
    "print('---')\n",
    "print(\"y2=\")\n",
    "print(y2[0])\n",
    "\n",
    "print(f\"Tensors are equal: {torch.allclose(y1, y2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "The final way we can change this is to use `Softmax`. Rather than just use a simple averaging, we will move to use softmax. This allows us to move away from equal weighting between each of the tokens and eventaully move to a technique that allows different weights. Softmax will ensure that when we do this, the weights will be normalised. For now however, it will achieve the same results as `matrix_based`. \n",
    "\n",
    "Note: In order to the softmax to normalise the future tokens to $0$, we mask them to be `-inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1=\n",
      "tensor([[1.4451, 0.8564],\n",
      "        [1.8316, 0.6898],\n",
      "        [1.3366, 0.3941],\n",
      "        [0.7388, 0.6151],\n",
      "        [0.5566, 0.5968],\n",
      "        [0.4733, 0.5684],\n",
      "        [0.4878, 0.3955],\n",
      "        [0.1510, 0.2522]])\n",
      "---\n",
      "y2=\n",
      "tensor([[1.4451, 0.8564],\n",
      "        [1.8316, 0.6898],\n",
      "        [1.3366, 0.3941],\n",
      "        [0.7388, 0.6151],\n",
      "        [0.5566, 0.5968],\n",
      "        [0.4733, 0.5684],\n",
      "        [0.4878, 0.3955],\n",
      "        [0.1510, 0.2522]])\n",
      "---\n",
      "y3=\n",
      "tensor([[1.4451, 0.8564],\n",
      "        [1.8316, 0.6898],\n",
      "        [1.3366, 0.3941],\n",
      "        [0.7388, 0.6151],\n",
      "        [0.5566, 0.5968],\n",
      "        [0.4733, 0.5684],\n",
      "        [0.4878, 0.3955],\n",
      "        [0.1510, 0.2522]])\n",
      "Tensors are equal: True\n"
     ]
    }
   ],
   "source": [
    "def softmax_based(x):\n",
    "    B, T, C = x.shape\n",
    "    tril = torch.tril(torch.ones(T, T))\n",
    "    weights = torch.zeros((T,T))\n",
    "    weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "    weights = F.softmax(weights, dim=-1)\n",
    "    return weights @ x\n",
    "\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "y1 = for_loop_based(x)\n",
    "y2 = matrix_based(x)\n",
    "y3 = softmax_based(x)\n",
    "\n",
    "print(\"y1=\")\n",
    "print(y1[0])\n",
    "print('---')\n",
    "print(\"y2=\")\n",
    "print(y2[0])\n",
    "print('---')\n",
    "print(\"y3=\")\n",
    "print(y3[0])\n",
    "\n",
    "print(f\"Tensors are equal: {torch.allclose(y1, y2) and torch.allclose(y1, y3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going to Self Attention!\n",
    "The next block of code improves on that of the `softmax_based` function and takes us to something that looks like a basic self attention block.\n",
    "\n",
    "Rather than create a weight matrix that determines the strength of the relationship between tokens that is based on averages (and therefore gives equal weighting to all previous tokens), we want to move to a technique that is more data driven. \n",
    "\n",
    "Given the sequence `['t', 'h', 'e', '<s>', 'c', 'a', 't', 's']`, when computing the self attention of the `<s>` token, the `'t'`, and `'e'` tokens will most likely have different weights assigned to them. However, the previous mechanisms we implemented won't account for this.\n",
    "\n",
    "To overcome this issue, we introduce three vectors:\n",
    "1. Query\n",
    "2. Key\n",
    "3. Value\n",
    "\n",
    "Which are computed by performing linear transformations (via `nn.Linear` with `bias=False`) on the input `x`. The query vector roughly represents a token saying *\"What am I looking for?\"*, whilst a key vector roughly represents a token saying *\"This is what I contain\"*. Finally, the value vector roughly represents the token saying *\"If you find me interesting, this is what I will give you\"*. Rather than give the token itself, it will return the value vector. (So `x` is \"private\" or a \"hidden\" vector, and `v` is what is provided).\n",
    "\n",
    "Remember that as the vectors are provided by separate `nn.Linear` modules, they will \"learn\" the outputs they need to provide via back propagation.\n",
    "\n",
    "The weight tensor that we created in previous cells is generated by the dot product of the query vector and the transposed key vector. (Note: The more complex transpose operation below is because we are working with batched tensors). If two vectors are well aligned, they will have a high dot product. This is where the softmax comes in, it will normalise the values between 0 and 1. However, it should be noted that if the softmax receives one very high value with the rest being low values, it will converge on 1-hot vectors. To prevent this, and ensure a better variance across the weights, it is scaled by $\\frac{1}{\\sqrt{HeadSize}}$\n",
    "\n",
    "Given that we still want a decoder self attention block, we'll mask out future tokens using the same lower triangular matrix as we did before. \n",
    "\n",
    "The parameter `head_size` ($d_k$) correlates to the dimension of the `query` and `key` vectors. In the Transformer paper, the `value` vector has a different dimension ($d_v$) but for simplicity sake, we keep it the same in this implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16 #d_k in the paper\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "def self_attention(x):\n",
    "    B, T, C = x.shape\n",
    "    \n",
    "    k = key(x) # (B, T, head_size)\n",
    "    q = query(x) # (B, T, head_size)\n",
    "\n",
    "    weights = q @ k.transpose(-2,-1) * (head_size**-0.5) # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "    # Make it a decoder by masking out future tokens\n",
    "    tril = torch.tril(torch.ones(T, T))\n",
    "    weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "    \n",
    "    weights = F.softmax(weights, dim=-1)\n",
    "    \n",
    "    v = value(x) # (B, T, head_size)\n",
    "    out = weights @ v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = self_attention(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
