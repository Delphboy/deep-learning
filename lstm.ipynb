{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set device to: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Set device to: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditCleanJokes(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_location: str,\n",
    "        sequence_length: int,\n",
    "    ):\n",
    "        self.file_location = file_location\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "    \n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv(self.file_location)\n",
    "        text = train_df['Joke'].str.cat(sep=' ')\n",
    "        return text.split(' ')\n",
    "    \n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]).to(DEVICE), # X\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length + 1]).to(DEVICE), # y\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 lstm_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_layers: int,\n",
    "                 vocab_size: int):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.lstm_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "    \n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(DEVICE),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(DEVICE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, hyperparameters):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=hyperparameters['batch_size'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=hyperparameters['learning_rate'])\n",
    "    \n",
    "    for epoch in range(hyperparameters[\"num_epochs\"]):\n",
    "        state_h, state_c = model.init_state(hyperparameters['sequence_length'])\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            \n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            \n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, prompt, next_words=10):\n",
    "    model.eval()\n",
    "    \n",
    "    words = prompt.split(' ')\n",
    "    \n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(DEVICE)\n",
    "        \n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        \n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        \n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 8.84078311920166}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 8.840360641479492}\n",
      "{'epoch': 0, 'batch': 2, 'loss': 8.837823867797852}\n",
      "{'epoch': 0, 'batch': 3, 'loss': 8.834922790527344}\n",
      "{'epoch': 0, 'batch': 4, 'loss': 8.830992698669434}\n",
      "{'epoch': 0, 'batch': 5, 'loss': 8.824763298034668}\n",
      "{'epoch': 0, 'batch': 6, 'loss': 8.820745468139648}\n",
      "{'epoch': 0, 'batch': 7, 'loss': 8.816546440124512}\n",
      "{'epoch': 0, 'batch': 8, 'loss': 8.810458183288574}\n",
      "{'epoch': 0, 'batch': 9, 'loss': 8.805150032043457}\n",
      "{'epoch': 0, 'batch': 10, 'loss': 8.797379493713379}\n",
      "{'epoch': 0, 'batch': 11, 'loss': 8.792781829833984}\n",
      "{'epoch': 0, 'batch': 12, 'loss': 8.789649963378906}\n",
      "{'epoch': 0, 'batch': 13, 'loss': 8.778308868408203}\n",
      "{'epoch': 0, 'batch': 14, 'loss': 8.770609855651855}\n",
      "{'epoch': 0, 'batch': 15, 'loss': 8.760207176208496}\n",
      "{'epoch': 0, 'batch': 16, 'loss': 8.732942581176758}\n",
      "{'epoch': 0, 'batch': 17, 'loss': 8.708758354187012}\n",
      "{'epoch': 0, 'batch': 18, 'loss': 8.682168960571289}\n",
      "{'epoch': 0, 'batch': 19, 'loss': 8.625129699707031}\n",
      "{'epoch': 0, 'batch': 20, 'loss': 8.559713363647461}\n",
      "{'epoch': 0, 'batch': 21, 'loss': 8.456242561340332}\n",
      "{'epoch': 0, 'batch': 22, 'loss': 8.418059349060059}\n",
      "{'epoch': 0, 'batch': 23, 'loss': 8.308427810668945}\n",
      "{'epoch': 0, 'batch': 24, 'loss': 8.229872703552246}\n",
      "{'epoch': 0, 'batch': 25, 'loss': 7.999321460723877}\n",
      "{'epoch': 0, 'batch': 26, 'loss': 8.016998291015625}\n",
      "{'epoch': 0, 'batch': 27, 'loss': 7.9056315422058105}\n",
      "{'epoch': 0, 'batch': 28, 'loss': 7.8778486251831055}\n",
      "{'epoch': 0, 'batch': 29, 'loss': 7.73972749710083}\n",
      "{'epoch': 1, 'batch': 0, 'loss': 7.413517475128174}\n",
      "{'epoch': 1, 'batch': 1, 'loss': 7.469663143157959}\n",
      "{'epoch': 1, 'batch': 2, 'loss': 7.535020351409912}\n",
      "{'epoch': 1, 'batch': 3, 'loss': 7.354711532592773}\n",
      "{'epoch': 1, 'batch': 4, 'loss': 7.299304008483887}\n",
      "{'epoch': 1, 'batch': 5, 'loss': 7.189333438873291}\n",
      "{'epoch': 1, 'batch': 6, 'loss': 7.213256359100342}\n",
      "{'epoch': 1, 'batch': 7, 'loss': 7.307733058929443}\n",
      "{'epoch': 1, 'batch': 8, 'loss': 7.154491901397705}\n",
      "{'epoch': 1, 'batch': 9, 'loss': 7.23543119430542}\n",
      "{'epoch': 1, 'batch': 10, 'loss': 7.102619647979736}\n",
      "{'epoch': 1, 'batch': 11, 'loss': 7.284687042236328}\n",
      "{'epoch': 1, 'batch': 12, 'loss': 7.383922576904297}\n",
      "{'epoch': 1, 'batch': 13, 'loss': 7.267764568328857}\n",
      "{'epoch': 1, 'batch': 14, 'loss': 7.325435638427734}\n",
      "{'epoch': 1, 'batch': 15, 'loss': 7.430271625518799}\n",
      "{'epoch': 1, 'batch': 16, 'loss': 7.2437849044799805}\n",
      "{'epoch': 1, 'batch': 17, 'loss': 7.209217548370361}\n",
      "{'epoch': 1, 'batch': 18, 'loss': 7.22332763671875}\n",
      "{'epoch': 1, 'batch': 19, 'loss': 7.144148826599121}\n",
      "{'epoch': 1, 'batch': 20, 'loss': 7.2390828132629395}\n",
      "{'epoch': 1, 'batch': 21, 'loss': 7.015341758728027}\n",
      "{'epoch': 1, 'batch': 22, 'loss': 7.1934967041015625}\n",
      "{'epoch': 1, 'batch': 23, 'loss': 7.209401607513428}\n",
      "{'epoch': 1, 'batch': 24, 'loss': 7.254927635192871}\n",
      "{'epoch': 1, 'batch': 25, 'loss': 6.960077285766602}\n",
      "{'epoch': 1, 'batch': 26, 'loss': 7.132066249847412}\n",
      "{'epoch': 1, 'batch': 27, 'loss': 7.035613059997559}\n",
      "{'epoch': 1, 'batch': 28, 'loss': 7.139063358306885}\n",
      "{'epoch': 1, 'batch': 29, 'loss': 6.9109907150268555}\n"
     ]
    }
   ],
   "source": [
    "dataset = RedditCleanJokes(\"/homes/hps01/deep-learning/datasets/reddit-clean-jokes.csv\", sequence_length=10)\n",
    "model = Lstm(lstm_size=256, \n",
    "             embedding_dim=256, \n",
    "             num_layers=2, \n",
    "             vocab_size=len(dataset.get_uniq_words()))\n",
    "model.to(DEVICE)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"batch_size\": 811,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_epochs\": 2,\n",
    "    \"sequence_length\": 10\n",
    "}\n",
    "\n",
    "trained_model = train(dataset, model, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'do', 'you', 'call', 'a', 'Rolling', 'terrible', 'of', 'day', 'protective', 'a', 'your', 'by', 'have', 'He']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What do you call a\"\n",
    "print(predict(dataset, trained_model, prompt, next_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
